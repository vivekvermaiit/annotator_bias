{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4760cb94-69f1-4585-89ff-66e33c5f07f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nubster/miniconda3/envs/comp550-final-project/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "## majority voting for the label\n",
    "## remove those that don't have majority\n",
    "## track f1 and accuracy through epochs, save model each epoch, choose the epoch that has max acc \n",
    "## Use those params to evaluate on test set. \n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "def load_data(data_path):\n",
    "    import json\n",
    "    with open(data_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    #add majority label here and remove those keys that don't have a majority label. \n",
    "    non_majority_label_keys = []\n",
    "    from collections import Counter\n",
    "    for key, value in data.items():\n",
    "        labels = [value['annotators'][0]['label'], value['annotators'][1]['label'], value['annotators'][2]['label']]\n",
    "        counter = Counter(labels)\n",
    "        most_common_labels = counter.most_common()\n",
    "        max_count = most_common_labels[0][1]\n",
    "        \n",
    "        # Get all labels with the max count (in case of a tie)\n",
    "        majority_labels = [label for label, count in most_common_labels if count == max_count]\n",
    "    \n",
    "        if len(majority_labels) > 1:\n",
    "            non_majority_label_keys.append(key)\n",
    "        else:\n",
    "            value['majority'] = majority_labels[0]\n",
    "            \n",
    "    for key in non_majority_label_keys:\n",
    "        data.pop(key, None)  # Avoids KeyError if key is missing\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Load the HateXplain dataset\n",
    "def load_hatexplain(data_path, post_id_divisions_path):\n",
    "    data = load_data(data_path)\n",
    "    \n",
    "    train_texts, train_labels, train_annotators, val_texts, val_labels, val_annotators, test_texts, test_labels, test_annotators = [], [], [], [], [], [], [], [], []\n",
    "    label_map = {\"hatespeech\": 0, \"offensive\": 1, \"normal\": 2, \"none\": 3}\n",
    "\n",
    "    post_id_divisions_path = \"Data/post_id_divisions.json\"\n",
    "    with open(post_id_divisions_path, \"r\") as f:\n",
    "        post_id = json.load(f)\n",
    "    \n",
    "    train_data = {k: data[k] for k in post_id[\"train\"] if k in data}\n",
    "    val_data = {k: data[k] for k in post_id[\"val\"] if k in data}\n",
    "    test_data = {k: data[k] for k in post_id[\"test\"] if k in data}\n",
    "\n",
    "    for key, value in train_data.items(): #repeat everything 3 times. \n",
    "        train_texts.append(\" \".join(value[\"post_tokens\"]))\n",
    "        train_labels.append(label_map[value[\"annotators\"][0][\"label\"]])\n",
    "        train_texts.append(\" \".join(value[\"post_tokens\"]))\n",
    "        train_labels.append(label_map[value[\"annotators\"][1][\"label\"]])\n",
    "        train_texts.append(\" \".join(value[\"post_tokens\"]))\n",
    "        train_labels.append(label_map[value[\"annotators\"][2][\"label\"]])\n",
    "\n",
    "        train_annotators.append(value[\"annotators\"][0][\"annotator_id\"])\n",
    "        train_annotators.append(value[\"annotators\"][1][\"annotator_id\"])\n",
    "        train_annotators.append(value[\"annotators\"][2][\"annotator_id\"])\n",
    "    for key, value in val_data.items():\n",
    "        val_texts.append(\" \".join(value[\"post_tokens\"]))\n",
    "        val_labels.append(label_map[value[\"annotators\"][0][\"label\"]])\n",
    "        val_texts.append(\" \".join(value[\"post_tokens\"]))\n",
    "        val_labels.append(label_map[value[\"annotators\"][1][\"label\"]])\n",
    "        val_texts.append(\" \".join(value[\"post_tokens\"]))\n",
    "        val_labels.append(label_map[value[\"annotators\"][2][\"label\"]])\n",
    "        \n",
    "        val_annotators.append(value[\"annotators\"][0][\"annotator_id\"])\n",
    "        val_annotators.append(value[\"annotators\"][1][\"annotator_id\"])\n",
    "        val_annotators.append(value[\"annotators\"][2][\"annotator_id\"])                              \n",
    "    for key, value in test_data.items():\n",
    "        test_texts.append(\" \".join(value[\"post_tokens\"]))\n",
    "        test_labels.append(label_map[value[\"annotators\"][0][\"label\"]])\n",
    "        test_texts.append(\" \".join(value[\"post_tokens\"]))\n",
    "        test_labels.append(label_map[value[\"annotators\"][1][\"label\"]])\n",
    "        test_texts.append(\" \".join(value[\"post_tokens\"]))\n",
    "        test_labels.append(label_map[value[\"annotators\"][2][\"label\"]])   \n",
    "        \n",
    "        test_annotators.append(value[\"annotators\"][0][\"annotator_id\"])\n",
    "        test_annotators.append(value[\"annotators\"][1][\"annotator_id\"])\n",
    "        test_annotators.append(value[\"annotators\"][2][\"annotator_id\"])                                 \n",
    "    return train_texts, train_labels, train_annotators, val_texts, val_labels, val_annotators, test_texts, test_labels, test_annotators\n",
    "\n",
    "# Custom PyTorch dataset class\n",
    "class HateSpeechDataset(Dataset):\n",
    "    def __init__(self, texts, annotators, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.annotators = annotators\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            \"annotator: {}\".format(self.annotators[idx]),\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Load dataset\n",
    "data_path = \"Data/dataset.json\"\n",
    "post_id_divisions_path = \"Data/post_id_divisions.json\"\n",
    "\n",
    "train_texts, train_labels, train_annotators, val_texts, val_labels, val_annotators, test_texts, test_labels, test_annotators = load_hatexplain(data_path, post_id_divisions_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "252ef6a6-09ab-4515-aa2d-9a3bf897623b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = HateSpeechDataset(train_texts, train_annotators, train_labels, tokenizer)\n",
    "val_dataset = HateSpeechDataset(val_texts, val_annotators, val_labels, tokenizer)\n",
    "test_dataset = HateSpeechDataset(test_texts, test_annotators, test_labels, tokenizer)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ee055aa-d72d-4b52-a6ef-a636637c3cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load BERT model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3, hidden_dropout_prob=0.1)\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Training loop with progress bar, accuracy, and F1-score\n",
    "def train_model(model, train_loader, val_loader, epochs=3, save_dir=\"saved_models\"):\n",
    "    best_val_acc = 0  # Track highest validation accuracy\n",
    "    best_model_path = None\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "        for batch in progress_bar:\n",
    "            input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['label'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Compute training loss and accuracy\n",
    "            total_loss += loss.item()\n",
    "            preds = outputs.logits.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Store predictions and labels for F1-score\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        # Compute metrics\n",
    "        train_acc = correct / total\n",
    "        train_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "        # Validation evaluation\n",
    "        val_acc, val_f1 = evaluate(model, val_loader)\n",
    "\n",
    "        # Save model for this epoch\n",
    "        model_path = os.path.join(save_dir, f\"model_epoch_{epoch+1}.pt\")\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        \n",
    "        # Track the best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_path = model_path  # Save best model path\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {total_loss/len(train_loader):.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "    print(f\"Best model saved at {best_model_path} with Val Acc: {best_val_acc:.4f}\")\n",
    "    last_model_path = os.path.join(save_dir, f\"model_epoch_{epochs}.pt\")\n",
    "    return best_model_path, last_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c788711a-519b-456c-b685-3a00a49cb72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                            \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accuracy, macro_f1\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Evaluate on test set after training\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFinal evaluation on test set:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 24\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, epochs)\u001b[0m\n\u001b[1;32m     22\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask)\n\u001b[1;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39mlogits, labels)\n\u001b[0;32m---> 24\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Compute training loss and accuracy\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/comp550-final-project/lib/python3.13/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/comp550-final-project/lib/python3.13/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/comp550-final-project/lib/python3.13/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def majority_vote(arr):\n",
    "    counter = Counter(arr)\n",
    "    most_common = counter.most_common()\n",
    "    \n",
    "    # Check if there's a clear majority\n",
    "    if len(most_common) == 1 or most_common[0][1] > most_common[1][1]:\n",
    "        return most_common[0][0]  # Return majority label\n",
    "    else:\n",
    "        return 3  # No majority, assign label 3\n",
    "\n",
    "def aggregate_predictions(all_preds, all_labels, group_size=3):\n",
    "    new_preds = []\n",
    "    new_labels = []\n",
    "\n",
    "    for i in range(0, len(all_preds), group_size):\n",
    "        group_preds = all_preds[i:i+group_size]\n",
    "        group_labels = all_labels[i:i+group_size]\n",
    "\n",
    "        new_preds.append(majority_vote(group_preds))\n",
    "        new_labels.append(majority_vote(group_labels))\n",
    "\n",
    "    return np.array(new_preds), np.array(new_labels)\n",
    "\n",
    "# Evaluation function with accuracy and F1-score\n",
    "def evaluate(model, data_loader, final=False):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['label'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            preds = outputs.logits.argmax(dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "# change this calc here. But check first that the labels are not shuffled and that you can club by groups of three\n",
    "# then you can do majority voting. \n",
    "# I printed the batch here and yeah this is not shuffled. \n",
    "\n",
    "    new_preds, new_labels = aggregate_predictions(all_preds, all_labels)\n",
    "    correct = (new_preds == new_labels).sum()\n",
    "    total = len(new_labels)\n",
    "    accuracy = correct / total\n",
    "    macro_f1 = f1_score(new_labels, new_preds, average='macro')\n",
    "\n",
    "    if final:\n",
    "        print(\"\\nFinal Test Classification Report:\\n\", classification_report(new_labels, new_preds))\n",
    "        num_threes_preds = np.sum(new_preds == 3)\n",
    "        num_threes_labels = np.sum(new_labels == 3)\n",
    "\n",
    "        print(\"Number of 3s in preds:\", num_threes_preds)\n",
    "        print(\"Number of 3s in labels:\", num_threes_labels)\n",
    "\n",
    "\n",
    "    return accuracy, macro_f1\n",
    "\n",
    "# Train the model\n",
    "best_model_path, last_model_path = train_model(model, train_loader, val_loader, epochs=20)\n",
    "\n",
    "\n",
    "# Evaluate on test set after training best model\n",
    "print(\"\\nFinal evaluation on test set best model:\")\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "test_acc, test_f1 = evaluate(model, test_loader, final=True)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}, Test Macro F1-score: {test_f1:.4f}\")\n",
    "\n",
    "# Evaluate on test set after training last model\n",
    "print(\"\\nFinal evaluation on test set last model:\")\n",
    "model.load_state_dict(torch.load(last_model_path))\n",
    "test_acc, test_f1 = evaluate(model, test_loader, final=True)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}, Test Macro F1-score: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07d2a51a-ad37-4637-9ae0-278344daa675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15383"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f6d8f0f-8f2c-4f66-9233-53368bbd2d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     annotator_id  count\n",
      "0               4   5730\n",
      "1               9   2075\n",
      "2             223   1978\n",
      "3             203   1918\n",
      "4             209   1769\n",
      "..            ...    ...\n",
      "248           192      3\n",
      "249           188      3\n",
      "250           174      3\n",
      "251           190      2\n",
      "252           138      1\n",
      "\n",
      "[253 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "with open(\"Data/dataset.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "annotator_ids = []\n",
    "for post in data.values():\n",
    "    annotator_ids.extend([annotator[\"annotator_id\"] for annotator in post[\"annotators\"]])\n",
    "\n",
    "# Convert to Pandas DataFrame and count occurrences\n",
    "df = pd.DataFrame({\"annotator_id\": annotator_ids})\n",
    "annotator_counts = df[\"annotator_id\"].value_counts().reset_index()\n",
    "annotator_counts.columns = [\"annotator_id\", \"count\"]\n",
    "\n",
    "# Display sorted annotators\n",
    "print(annotator_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9b9c3a9-0197-477d-835b-a2b11a057f34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(60444)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotator_counts[\"count\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ddd6a73-2f73-49a1-9e0d-8ac7c4456528",
   "metadata": {},
   "outputs": [],
   "source": [
    "x={\"post_id\": \"1178783381934526465_twitter\", \"annotators\": [{\"label\": \"normal\", \"annotator_id\": 17, \"target\": [\"None\"]}, {\"label\": \"normal\", \"annotator_id\": 4, \"target\": [\"Homosexual\"]}, {\"label\": \"offensive\", \"annotator_id\": 38, \"target\": [\"Women\", \"Indigenous\", \"Caucasian\", \"Nonreligious\", \"Homosexual\"]}], \"rationales\": [], \"post_tokens\": [\"sabine\", \"wren\", \"is\", \"a\", \"raging\", \"queer\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "067101e0-f40d-4f3a-a64b-be1c59c739a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sabine wren is a raging queer 17'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a214b36-d7d4-4677-987e-f6269fcc72cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[\"annotators\"][0][\"annotator_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d19ed75-f4d9-448e-a369-5a9d2e7f2e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sabine wren is a raging queer'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(x[\"post_tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "717e37e6-e379-495f-98eb-0cb5b478d365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101,  1057,  2428,  2228,  1045,  2052,  2025,  2031,  2042, 15504,\n",
       "          2011, 18993,  7560,  2030,  5152,  2067,  1999,  2634,  2030,  7269,\n",
       "          1998,  1037,  9253,  6394,  2052,  9040,  2033,  2004,  2092,  2074,\n",
       "          2000,  2156,  2033,  5390,   102,  5754, 17287,  4263,  1024, 18540,\n",
       "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'label': tensor(1)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "209b0b08-9cb5-4295-8800-84216b4bd50a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] u really think i would not have been raped by feral hindu or muslim back in india or bangladesh and a neo nazi would rape me as well just to see me cry [SEP] annotator : 203 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_dataset[0][\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b31fd0e9-cf8d-4c3b-829d-bb9405f9bef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Predictions: [1 2 3]\n",
      "New Labels: [1 2 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def majority_vote(arr):\n",
    "    counter = Counter(arr)\n",
    "    most_common = counter.most_common()\n",
    "    \n",
    "    # Check if there's a clear majority\n",
    "    if len(most_common) == 1 or most_common[0][1] > most_common[1][1]:\n",
    "        return most_common[0][0]  # Return majority label\n",
    "    else:\n",
    "        return 3  # No majority, assign label 3\n",
    "\n",
    "def aggregate_predictions(all_preds, all_labels, group_size=3):\n",
    "    new_preds = []\n",
    "    new_labels = []\n",
    "\n",
    "    for i in range(0, len(all_preds), group_size):\n",
    "        group_preds = all_preds[i:i+group_size]\n",
    "        group_labels = all_labels[i:i+group_size]\n",
    "\n",
    "        new_preds.append(majority_vote(group_preds))\n",
    "        new_labels.append(majority_vote(group_labels))\n",
    "\n",
    "    return np.array(new_preds), np.array(new_labels)\n",
    "\n",
    "# Example usage\n",
    "all_preds = np.array([0, 1, 1, 2, 2, 2, 0, 1, 2])\n",
    "all_labels = np.array([0, 1, 1, 2, 2, 1, 0, 1, 1])\n",
    "\n",
    "new_preds, new_labels = aggregate_predictions(all_preds, all_labels)\n",
    "print(\"New Predictions:\", new_preds)\n",
    "print(\"New Labels:\", new_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e08709e-cae2-4b1f-ad71-467d2c55c8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = (new_preds == new_labels).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9be6aa3-1b6a-41fb-8ff4-e652e4eb30a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc24027-dc01-40a2-b2e6-2ad93f5ec857",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp550-final-project",
   "language": "python",
   "name": "comp550-final-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
